{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T20:57:04.175209Z",
     "start_time": "2024-05-07T20:56:54.958534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, AdamW\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from tqdm import tqdm"
   ],
   "id": "bb2370c79301f95",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rouge_metric = load_metric('rouge')\n",
    "bleu_metric = load_metric('bleu')\n",
    "\n",
    "dataset = load_dataset(\"bavard/personachat_truecased\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')  #gpt2-medium\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "id": "86f9803f1d18c509"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Concatenate 'persona', 'history', and 'candidates' to form the input text\n",
    "def chunking(examples):\n",
    "    inputs = [\n",
    "        tokenizer.bos_token + \"\\n-----\\n\".join(history) + \"\\n-----\\n\" + candidate + tokenizer.eos_token\n",
    "        for history, candidates in zip(examples[\"history\"], examples[\"candidates\"])\n",
    "        for candidate in candidates[-2:-1]\n",
    "    ]\n",
    "    return {\"chunks\": inputs}"
   ],
   "id": "88dc7cf2ac16845d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def tokenize(examples):\n",
    "    input_data = tokenizer(examples[\"chunks\"], padding='max_length', truncation=True, max_length=256)\n",
    "    outputs = {\n",
    "        \"input_ids\": input_data['input_ids'],\n",
    "        \"attention_mask\": input_data['attention_mask'],\n",
    "    }\n",
    "    return outputs"
   ],
   "id": "3b0a70d83ad81f6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenized_datasets = (\n",
    "    dataset\n",
    "    .map(chunking, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "    .map(tokenize, batched=True, remove_columns=[\"chunks\"])\n",
    ")"
   ],
   "id": "4ace4313b295eecb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
    "valid_dataset = tokenized_datasets[\"validation\"]"
   ],
   "id": "9d5da76640f2ae36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset.select(list(range(1000))),\n",
    "    shuffle=False,\n",
    "    batch_size=8,\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset.select(list(range(1000))),\n",
    "    shuffle=True,\n",
    "    batch_size=8,\n",
    ")"
   ],
   "id": "633904b5787437d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "id": "408b300fc6f26a0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_bleu_score(generated_texts, reference_texts):\n",
    "    return bleu_metric.compute(predictions=generated_texts, references=reference_texts)"
   ],
   "id": "c1a527895f118464"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def calculate_rouge_scores(generated_texts, reference_texts):\n",
    "    rouge_hypotheses = generated_texts\n",
    "    rouge_references = [{\"summary\": ref} for ref in reference_texts]\n",
    "    rouge_results = rouge_metric.compute(predictions=rouge_hypotheses, references=rouge_references)\n",
    "    return rouge_results"
   ],
   "id": "3ca999254e2b8038"
  },
  {
   "metadata": {
    "id": "e6aabae76d3222aa"
   },
   "cell_type": "code",
   "source": [
    "def generate_text(input_ids, attention_mask, num_return_sequences=1, max_length=513, temperature=1.0, top_k=50):\n",
    "    output = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_length,\n",
    "                            num_return_sequences=num_return_sequences,\n",
    "                            temperature=temperature, top_k=top_k, repetition_penalty=5.0,\n",
    "                            pad_token_id=tokenizer.eos_token_id)\n",
    "    generated_texts = []\n",
    "    for generated in output:\n",
    "        text = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "        generated_texts.append(text)\n",
    "    return generated_texts"
   ],
   "id": "e6aabae76d3222aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "cfef0b0c7367e203",
    "outputId": "ab5b5a34-301f-49dc-c1a1-8e3a44b29d4c"
   },
   "cell_type": "code",
   "source": [
    "model.train()\n",
    "\n",
    "num_epochs = 10\n",
    "accumulation_steps = 4  # Accumulate gradients over 4 steps before updating\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}', leave=False)\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        output_ids = batch[\"input_ids\"].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=output_ids)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Perform gradient accumulation\n",
    "        loss = loss / accumulation_steps  # Scale loss for gradient accumulation\n",
    "        loss.backward()\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "            # Update model parameters after accumulation_steps or at the end of epoch\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'Loss': total_loss / (step + 1)})\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1} Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    valid_progress_bar = tqdm(valid_dataloader, desc=f'Epoch {epoch + 1}', leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_bleu_score = 0\n",
    "        total_rouge_scores = {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "\n",
    "        for step, batch in enumerate(valid_progress_bar):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            generated_texts = generate_text(input_ids, attention_mask=attention_mask, num_return_sequences=1,\n",
    "                                            max_length=512)\n",
    "\n",
    "            # BLEU evaluation\n",
    "            bleu_score = calculate_bleu_score(generated_texts, batch[\"input_ids\"])\n",
    "            total_bleu_score += bleu_score\n",
    "\n",
    "            # ROUGE evaluation\n",
    "            rouge_scores = calculate_rouge_scores(generated_texts, batch[\"input_ids\"])\n",
    "            for key, value in rouge_scores.items():\n",
    "                if key in total_rouge_scores:\n",
    "                    total_rouge_scores[key] += value.mid.fmeasure\n",
    "            valid_progress_bar.set_postfix({\n",
    "                'Val_bleu_score': total_bleu_score / (step + 1),\n",
    "                'Val_rouge1': total_rouge_scores['rouge1'] / (step + 1),\n",
    "                'Val_rouge2': total_rouge_scores['rouge2'] / (step + 1),\n",
    "                'Val_rougeL': total_rouge_scores['rougeL'] / (step + 1)\n",
    "            })\n",
    "\n",
    "        avg_bleu_score = total_bleu_score / len(valid_dataloader)\n",
    "        avg_rouge_scores = {key: value / len(valid_dataloader) for key, value in total_rouge_scores.items()}\n",
    "\n",
    "        print(\"Validation Results - Epoch {}: BLEU: {:.4f}, ROUGE: {}\".format(epoch, avg_bleu_score, avg_rouge_scores))\n",
    "\n",
    "    model.train()"
   ],
   "id": "cfef0b0c7367e203",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "ba779bb90bcbc1c0",
    "ExecuteTime": {
     "end_time": "2024-05-04T17:53:49.487430Z",
     "start_time": "2024-05-04T17:53:49.002106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_path = 'GPT2-persona-model-new.pth'\n",
    "torch.save(model.state_dict(), output_path)"
   ],
   "id": "ba779bb90bcbc1c0",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "id": "645b0d6c30f2348f",
    "ExecuteTime": {
     "end_time": "2024-05-07T20:57:18.923231Z",
     "start_time": "2024-05-07T20:57:15.402359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_path = 'GPT2-persona-model-new.pth'\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n"
   ],
   "id": "645b0d6c30f2348f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "id": "67504e1b7fb1f01c",
    "outputId": "f1b4dc4b-03e9-4a93-9c73-62dea1fbf916",
    "ExecuteTime": {
     "end_time": "2024-05-07T22:41:26.097064Z",
     "start_time": "2024-05-07T22:41:23.324817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Example prompt\n",
    "prompt = (tokenizer.bos_token + \"Hello, my name is Sasha. What's your name? And how are you?\" + \"\\n-----\\n\")\n",
    "# prompt = (tokenizer.bos_token + \"Hi. What brought you here?\" + \"\\n-----\\n\")\n",
    "\n",
    "# Generate multiple responses\n",
    "num_return_sequences = 3\n",
    "generated_responses = []\n",
    "for _ in range(num_return_sequences):\n",
    "    # Generate one response at a time\n",
    "    input_data = tokenizer(prompt, truncation=True, padding=True, max_length=256)\n",
    "\n",
    "    input_ids = torch.tensor([input_data['input_ids']])\n",
    "    attention_mask = torch.tensor([input_data['attention_mask']])\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    generated_response = model.generate(input_ids=input_ids, attention_mask=attention_mask,max_new_tokens=64,\n",
    "                                        num_return_sequences=1, top_k=50, repetition_penalty=1.02,\n",
    "                                        pad_token_id=tokenizer.eos_token_id)\n",
    "    generated_text = tokenizer.decode(generated_response[0], skip_special_tokens=True)\n",
    "    generated_responses.append(generated_text)\n",
    "    prompt = generated_text.replace('__ SILENCE __',\"\")\n",
    "    \n",
    "# Print the generated responses\n",
    "for i, response in enumerate(generated_responses):\n",
    "    print(f\"Generated Response {i + 1}: {response}\")"
   ],
   "id": "67504e1b7fb1f01c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response 1: Hello, my name is Sasha. What's your name? And how are you?\n",
      "-----\n",
      "Hi! I am jessica. How are you doing today?\n",
      "-----\n",
      "I'm good thanks for asking. Just got done with a long day at work.\n",
      "-----\n",
      "That sounds like fun. Do you have any hobbies?\n",
      "-----\n",
      "Yes, but mostly just watching movies and reading books. You?\n",
      "Generated Response 2: Hello, my name is Sasha. What's your name? And how are you?\n",
      "-----\n",
      "Hi! I am jessica. How are you doing today?\n",
      "-----\n",
      "I'm good thanks for asking. Just got done with a long day at work.\n",
      "-----\n",
      "That sounds like fun. Do you have any hobbies?\n",
      "-----\n",
      "Yes, but mostly just watching movies and reading books. You?\n",
      "-----\n",
      "I love to read too. My favorite book is the bible.\n",
      "-----\n",
      "Oh wow that's very interesting. Are you married?\n",
      "-----\n",
      "No, not yet. But I do plan on starting a new life soon.\n",
      "-----\n",
      "What kind of music do you like?\n",
      "Generated Response 3: Hello, my name is Sasha. What's your name? And how are you?\n",
      "-----\n",
      "Hi! I am jessica. How are you doing today?\n",
      "-----\n",
      "I'm good thanks for asking. Just got done with a long day at work.\n",
      "-----\n",
      "That sounds like fun. Do you have any hobbies?\n",
      "-----\n",
      "Yes, but mostly just watching movies and reading books. You?\n",
      "-----\n",
      "I love to read too. My favorite book is the bible.\n",
      "-----\n",
      "Oh wow that's very interesting. Are you married?\n",
      "-----\n",
      "No, not yet. But I do plan on starting a new life soon.\n",
      "-----\n",
      "What kind of music do you like?\n"
     ]
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
